version: '3.8'

services:
  # ============================================================================
  # ELK STACK - Logging Infrastructure
  # ============================================================================
  
  elasticsearch:
    image: docker.io/elasticsearch:8.11.0
    container_name: techpulse-elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data:Z
    networks:
      - techpulse-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    image: docker.io/logstash:8.11.0
    container_name: techpulse-logstash
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "5000:5000"
      - "9600:9600"
    volumes:
      - ./elk/logstash/pipeline:/usr/share/logstash/pipeline:Z
      - ./elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:Z
    networks:
      - techpulse-network
    environment:
      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"

  kibana:
    image: docker.io/kibana:8.11.0
    container_name: techpulse-kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    networks:
      - techpulse-network

  filebeat:
    image: docker.io/elastic/filebeat:8.11.0
    container_name: techpulse-filebeat
    user: root
    depends_on:
      - logstash
    volumes:
      - ./elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:Z,ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat-data:/usr/share/filebeat/data:Z
    command: filebeat -e -strict.perms=false
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=filebeat,env=production"

  # ============================================================================
  # EXISTING SERVICES (with logging configuration)
  # ============================================================================

  zookeeper:
    image: docker.io/confluentinc/cp-zookeeper:7.5.0
    container_name: techpulse-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=zookeeper,env=production"

  kafka:
    image: docker.io/confluentinc/cp-kafka:7.5.0
    container_name: techpulse-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data:Z
      - ./kafka-config:/kafka-config:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=kafka,env=production"

  postgres:
    image: docker.io/postgres:16-alpine
    container_name: techpulse-postgres
    environment:
      POSTGRES_DB: techpulse
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: changeme
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data:Z
      - ./database/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:Z
    networks:
      - techpulse-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d techpulse"]
      interval: 10s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=postgres,env=production"

  redis:
    image: docker.io/redis:7-alpine
    container_name: techpulse-redis
    command: redis-server --requirepass changeme
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=redis,env=production"

  qdrant:
    image: docker.io/qdrant/qdrant:v1.7.4
    container_name: techpulse-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant-data:/qdrant/storage:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=qdrant,env=production"

  scraper:
    build:
      context: ./scrapers
      dockerfile: ../Containerfile.scraper
    container_name: techpulse-scraper
    depends_on:
      - kafka
      - logstash
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
    volumes:
      - ./scrapers:/app:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=scraper,env=production"

  dedup-service:
    build:
      context: ./services/deduplication
      dockerfile: ../../Containerfile.dedup
    container_name: techpulse-dedup
    depends_on:
      - kafka
      - redis
      - logstash
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      REDIS_HOST: redis
      REDIS_PASSWORD: changeme
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=dedup,env=production"

  embedding-service:
    build:
      context: ./services/embedding
      dockerfile: ../../Containerfile.embedding
    container_name: techpulse-embedding  
    depends_on:
      - kafka
      - logstash
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      OLLAMA_HOST: host.containers.internal
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
    extra_hosts:
      - "host.containers.internal:host-gateway"  
    networks:
      - techpulse-network
    deploy:
      replicas: 2
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=embedding,env=production"

  indexer-service:
    build:
      context: ./services/indexer
      dockerfile: ../../Containerfile.indexer
    container_name: techpulse-indexer
    depends_on:
      - kafka
      - qdrant
      - postgres
      - logstash
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      QDRANT_HOST: qdrant
      POSTGRES_HOST: postgres
      POSTGRES_DB: techpulse
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: changeme
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=indexer,env=production"

  backend:
    build:
      context: ./backend
      dockerfile: ../Containerfile.backend
    container_name: techpulse-backend
    depends_on:
      - postgres
      - redis
      - qdrant
      - logstash
    ports:
      - "8000:8000"
    environment:
      OLLAMA_HOST: host.containers.internal
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      QDRANT_HOST: qdrant
      REDIS_HOST: redis
      POSTGRES_HOST: postgres
      MOCK_EMBEDDINGS: "false"
      MOCK_LLM: "false"
      LOGSTASH_HOST: logstash
      LOGSTASH_PORT: 5000
    extra_hosts:
      - "host.containers.internal:host-gateway"
    volumes:
      - ./backend:/app:Z
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=backend,env=production"

  frontend:
    build:
      context: ./frontend
      dockerfile: ../Containerfile.frontend
    container_name: techpulse-frontend
    depends_on:
      - backend
    ports:
      - "3000:3000"
    environment:
      NEXT_PUBLIC_API_URL: http://localhost:8000
    networks:
      - techpulse-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=frontend,env=production"

networks:
  techpulse-network:
    driver: bridge

volumes:
  zookeeper-data:
  kafka-data:
  postgres-data:
  redis-data:
  qdrant-data:
  elasticsearch-data:
  filebeat-data: